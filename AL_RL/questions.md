# 面试题总结

## 自我介绍

面试官您好, 我叫从聪, 来自中科院软件研究所. 研究方向是强化学习, 目前在做的主要是交流式的协作多智能体RL问题, 在投一篇相关的论文. 在竞赛方面, 我在本科期间多次参与数学建模竞赛, 全国计算机博弈锦标赛六子棋项目等, 取得了一些成绩. 

在项目方面, 20年9月到21年2月, 我参与了实验室与国防大学合作的兵棋推演模型内置基线AI的开发工作, 如果您不了解兵棋可以把它理解成一种复杂的沙盘推演游戏. 我负责宏动作的封装和RL智能体的实现, 主要是基于各种RL算法和相关trick, 比如PPO-目标函数, 多头网络, 注意里机制等实现了一个即时策略的智能体AI. 因为这个他们比较想要实现一种模仿人类的长期规划能力的AI, 后来我又给他们基于对战池数据搜索的planning模型. 该项目顺利完成验收, 另外还在全军的兵棋对抗半决赛里取得了战胜人类对手的成绩, 最终的验收评级是优. 

在实习方面, 18年的时候在一点资讯实习过半年, 职位是用户增长算法部门的机器学习实习生. 当时的主要工作是负责本地推荐逻辑优化, 重训线上模型, 还有一些基础的统计调研工作.
我目前暑期实习想要找一份强化学习算法相关的工作.

## 论文介绍

我的工作只要是关于交流式多智能体协作问题.
首先说多智能体RL是POMDP问题. 最主要的想法是怎么加入更多的信息辅助Actor学习. 没有交流的一般用中心化训练分布式执行框架, 比如在训练的时候, 有一个中心化critic可以使用全局信息计算联合Q值, 其他智能体用自己的局部观察计算局部Q值或动作. 它的核心思想是在训练的时候使用更多的知识训练actor.

首先交流这个行为在人和动物合作的时候很普遍, 核心的思想就是增加关于部分可观察环境的知识, 一般做法都是把其他智能体GRU网络提取的特征拼接到自己的隐藏层特征后边, 过一个全连接层辅助动作分类.
交流问题一般关注三个问题: 什么时候, 和谁, 交流内容; 现在普遍使用的方法大概三个: 直接把所有的消息做平均分给所有智能体, 加权, gating, 这些算法一般都是假设连续交流的, 有的是连续收发, 有的连续收, 有的连续发.

我关注的问题是什么时候交流, 并且降低通信代价.
首先确定什么时候交流我想的是使用gating机制, 然后关于通信代价有两方面考虑, 一个是降低消息整合的代价, 这个可以用一个公共交流信道; 另一个比较重要的想法是我想把连续的交流变成不连续的交流. 所以我的核心motivation就是解决这三个问题.

我的方法是把通信考虑成一个单独学习的策略, 叫通信策略, 和行为策略并列学习, 这就是一个多任务学习的问题了, 比较直观能想到的就是让Actor输出一个多头, 一个头输出动作, 一个头输出交流策略, 他们公用底层结构, 这样底层GRU可以学习拟合两种策略. 然后为了打破这种连续通信, 我用类似DQN那种跳帧或者叠帧的思想, 因为对于交流策略来说, 它不需要更新的特别频繁, 在相似的状态或者相邻的时间步, 可以让交流策略保持不变. 这样, 我就让交流策略是每隔D步更新一次, 然后在接下来的这D步保持策略不变, 然后行动策略就是正常的单步更新就好了. 这个间隔就反应了交流的频率, D越大交流频率越低. 
最后就根据交流策略输出作为gating信号控制消息发送接受就好了, 这样就完全不是连续交流了.

最后把交流策略一种独立的动作还有一个好处, 就是可以直接加到现在已有的那些多智能体架构里边去, 我做实验的时候是用的QMIX网络.

实验使用SMAC 6个场景里, 每2000回合测试20回合. 分析的时候用胜率评价. 主要分析了正常交流频率下我的算法和baseline的对比, tarmac, qmix, 全交流版qmix; 在消融实验里分析了不同交流频率的影响, 还有同质异质智能体, 智能体数量的影响.

比较有意思的结论:

1. 同质智能体稍微多点里，不连续交流比完全不交流或者完全交流都好。完全交流会有过拟合现象。
2. 异质智能体数量稍微多点，会让算法性能下降，他们关注的点可能不同.
3. 智能体数量少的时候，交流比较重要。

## 项目

动作空间主要是控制各个实体的操作, 因为这个项目关注的是战术级的指挥, 不需要太过关注实体过于细节的东西.
动作主要包括移动类的, 攻击类, 向上级部队整合类, 因为包含海陆空三类的实体, 为了能统一动作表示, 所以得把原始接口封装成能三种实体统一调度的环境接口, 我就是按照前边说的这三类封装的动作. 比如对于移动类, 需要把空军的侦擦、转场、陆军的机动, 海军的机动, 都封装成一个宏动作. 留出己方实体和敌方实体的接口. 然后对于己方和敌方实体选择那块我跟您说过了. 这样的话降低了动作空间.
另外一点就是输入, 输入就是三类信息. 一个是战场局势地图, 我一个同学根据军方提供的军事资料, 根据武器、范围之类的参数将地图抽象成8x128x128的云图, 核心思想是把能力做成辐射图、敌方能力是己方威胁; 另外就是己方实体向量和敌方实体向量.
算法框架的话, 基本就是比较通用的处理游戏的框架了, 比alphaStar的框架做了简化, 一个是向量过全连接层, 地图过CNN, 然后把隐层拼起来过LSTM, 输出动作

## 对未来的规划

现阶段主要还是以学习为主. 我目前研究重点可能在游戏AI比较多. 但是通过最近开始找工作, 确实感觉RL落地的难度. 我的理解是在商业化应用里做推荐可能是RL比较有价值的方向, 像阿里和滴滴也都在做相关的研究. 

我以后想去互联网公司, 所以可能更倾向于在实习期间接触一些RL做推荐的实际应用的系统, 拓展一下自己的视野, 如果能顺利通过(字节)()秋招的话, 还是想要能够彻底熟悉一块业务, 未来承担核心任务.

## 问面试官

1. 请问您在面试时最看重哪方面的能力或者特质.
2. 部门业务, 如果我通过了主要负责哪方面?
3. 对新人培养方式, 如何帮助实习生快速上手项目
4. 其实我现在还是比较迷茫的, 请问有哪些建议.
5.  
