1. 连续动作空间为什么需要策略梯度方法?
在连续动作空间, **贪婪策略提升因为需要在每一步都需要全局最大值**, 所以很困难. 此时经常使用策略梯度方式进行改进, 沿Q值梯度的方向进行提升.
2. VPG,TRPO,PPO等算法使用on-policy, 也就是不适用老样本, 使得样本效率低, 但是这些算法的数学原理比较好, 直接优化策略性能目标函数. 所以更关注稳定性, 而不是样本效率. 他们一系列的改进都是为了提高样本效率. 
3. DDPG->TD3->SAC, 使用off-policy, 这些算法样本效率高, 但是不稳定.
4. 强化学习是什么? 
   RL研究通过试错法对智能体进行训练, 通过奖励信号引导智能体, 使其可以重复执行有用的动作, 放弃无用的动作.
5. **为什么Q-learning不稳定**? 
   [这篇论文](http://web.mit.edu/jnt/www/Papers/J063-97-bvr-td.pdf)还有suttonRLbook11.3节讨论了, function approximation, bootstrapping, off-policy data, 这三者结合造成了值函数学习算法的不稳定性.