# 2. 派单-DRL-with-knowledge-transger

paper: [Deep Reinforcement Learning with Knowledge Transfer for Online Rides Order Dispatching](https://ieeexplore.ieee.org/document/8594886)

核心思想:

- 将派单问题建模为 MDP, 使用带动作搜索的 DQN 方法进行优化.
- 大规模调度问题需要支持大量不同供需关系的地位位置. 为了提升算法适应性和效率, 提出一种**相关特征渐进迁移**的迁移学习方法, 同时使用两种已有方法同时在时域和空间域进行迁移.
- 实验证明当然了方法的最优性; 同时发现带迁移学习的方法 (不同城市之间知识迁移或者同时城市内不同时间/空间的知识迁移) 比不带知识迁移的方法好.

# 1 Introduction

派单两方面问题:

- 提升派单效率. 以前方法关注派单的距离和接客时间成本[7], 或者通过组合优化方法提升匹配的准确率[8], 或者为了提升收入使用 RL 方法辅助决策巡航路线[10]. [10]中的 RL 方法利用历史轨迹信息, 可以提升巡航效率, 但是只考虑空车的位置状态, 而没考虑接客之后司机位置变化情况, 因此如果数据集只有空车巡航信息, 仍然限制了全局优化性能.
- 扩展性. 城市之间的扩展. 虽然不同城市应该用不同模型, 但是还是有一些共同知识可以共享, 比如高峰时段交通需求. 因此可以使用迁移学习迁移知识.

本文, **从单个司机视角建模**, 司机被分配到一系列出行请求, 目标为最大化整天收入. [20]通过学习和规划的方法已经验证了这种方法的有效性. [20]的缺陷有三方面: 1) 状态价值随不同的实时因素变化巨大, 比如供需关系, 但是这种上下文信息对于表格方法很难解决. 换句话说, 智能体必须学习历史数据之外的信息. 2) 不同城市的 MDPs 有很多公用的结构. 但是表格方法对每个 mdp 单独对待, 无法进行知识迁移. 3) 策略提升是一个单独的步骤, 至少需要在线执行一天. 为了收敛,必须进行重复进行 evaluation-improvement 的迭代, 需要好几周时间. 因此需要才采用 off-policy 方法进行优化.

本文使用具有动作搜索能力的 DQN 方法. 关键是如何评估每一步司机的 Q 值. 使用 DQN 可以结合历史数据和多种辅助数据, 也可以跨多个城市加速学习. 通过迁移学习可以加速前期策略学习. 好处: 加速开始阶段学习, 收敛速度快, 收敛性能更高.
