# 1 百面ML-3经典算法

## 1 LR算法

### 1.1 原理

#### 基本原理

1. 通过sigmoid函数将样本x预测值映射到(0, 1)之间, 并且在做分类任务时, 将$$e^{wx+b}/1+e^{wx+b}$$作为分类为1的概率, $$1/1+e^{wx+b}$$作为分类为负样本的概率.

2. LR模型的基本假设还是 **模型输出Y=1的对数几率时关于x的线性函数**. 即

   $$log \frac{P(Y=1|x)}{1-P(Y=1|x)}=w \cdot x$$

#### 训练

在训练时, LR的loss可以使用极大似然估计

<div style="text-align: center; width: 80%; margin: auto; ">
<img width=100% src="img/2021_06_02_21_26_46.png">
</div>

为了更方便求解, 我们对等式两边同取对数, 写成对数似然函数:

<div style="text-align: center; width: 80%; margin: auto; ">
<img width=100% src="img/2021_06_02_21_31_17.png">
</div>
在机器学习中我们有损失函数的概念, 其衡量的是模型预测错误的程度. 如果取整个数据集上的平均对数似然损失, 我们可以得到:

$$J(w) = \frac{1}{N} ln L(w)$$

即在逻辑回归模型中, 我们最大化似然函数和最小化损失函数实际上是等价的.

#### 随机梯度下降法求解

<div style="text-align: center; width: 80%; margin: auto; ">
<img width=100% src="img/2021_06_02_21_34_08.png">
</div>

#### 牛顿法

牛顿法的基本思路是, 在现有极小点估计值的附近对 f(x) 做二阶泰勒展开, 进而找到极小点的下一个估计值. 假设 $$w^k$$ 为当前的极小值估计值, 那么有:

<div style="text-align: center; width: 80%; margin: auto; ">
<img width=100% src="img/2021_06_02_21_35_01.png">
</div>

### 1.2 线性回归和逻辑回归的异同

1. 线性回归做回归，因变量是离散的；
2. LR可以不符合线性关系.
3. 逻辑回归因变量时离散的，线性回归因变量是连续的。
4. 两者都用极大似然估计建模，也都可以用梯度下降方法求解。

### 1.3 优缺点

1. 优点
   - 实现简单, 计算代价不高, 易于理解和实现, 广泛的应用于工业问题上; 
   - 分类时计算量非常小, 速度很快, 存储资源低; 
   - 便利的观测样本概率分数; 
   - 对逻辑回归而言, 多重共线性并不是问题, 它可以结合L2正则化来解决该问题; 
2. 缺点
   - 当特征空间很大时, 逻辑回归的性能不是很好; 
   - 容易欠拟合, 一般准确度不太高
   - 不能很好地处理大量多类特征或变量; 
   - 只能处理两分类问题(在此基础上衍生出来的softmax可以用于多分类), 且必须线性可分; 
   - 对于非线性特征, 需要进行转换; 

## 2 决策树

特征选择, 树的构造, 树的剪枝

### 2.1 决策树的启发函数

1. ID3：最大信息增益
   - 倾向于选取值多的特征
   - 只能处理离散变量, 只能用于分类, 对缺失值敏感, 可多分支
2. C4.5：最大信息增益
   - 对ID3进行了优化, 平衡了分类的问题, 对取值比较多的特征进行了惩罚, 在ID3基础上除以取值熵.
   - 可以处理连续变量, 进行切分, 只能用于分类, 可多分支
3. CART：最大基尼指数
   - 二元树, 当基尼指数降为0, 完成决策树的生长.
   - 可以处理连续变量, 二值划分, 可用于回归, 只能二分, 特征可复用.
