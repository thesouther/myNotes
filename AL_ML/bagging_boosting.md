# 0 ML-集成学习

## GBDT和随机森林

1. 相同点：

   - 都是由多棵树组成; 
   - 最终的结果都是由多棵树一起决定

2. GBDT和随机森林的不同点:

   - 组成随机森林的树可以是分类树, 也可以是回归树; 而GBDT只由回归树组成; 
   - 组成随机森林的树可以并行生成; 而GBDT只能是串行生成; 
   - 对于最终的输出结果而言, 随机森林采用多数投票等; 而GBDT则是将所有结果累加起来, 或者加权累加起来; 
   - 随机森林对异常值不敏感, GBDT对异常值非常敏感; 
   - 随机森林对训练集一视同仁, GBDT是基于权值的弱分类器的集成; 
   - 随机森林是通过减少模型方差提高性能, GBDT是通过减少模型偏差提高性能.

## bagging

对给定有N个样本的数据集D进行Bootstrap采样, 得到$$D^1$$, 在$$D^1$$上训练模型$$f_1$$; 
上述过程重复M次, 得到M个模型, 则M个模型的平均(回归)/投票(分类)为

$$f_{avg}(x)=1/M \sum_{m=1}^M f_m(x)$$

可以证明:**Bagging可以降低模型的方差.**

## AdaBoost

在前一个弱学习器失败样本上学习下一个学习器, 对分错的样本权重增加.
<img src="img/2021_05_11_23_15_02.png">
