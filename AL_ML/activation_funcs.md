# 0 ML-激活函数

## 目录

神经网络中的激活函数就是为了增加非线性，激活函数选择了一个阈值，即当大于某个值就被激活，小于等于则输出 0。

激活函数也可以理解为对特征空间进行仿射变换（空间映射），在其中寻找线性边界。

1. Sigmoid
2. Tanh
3. ReLU
4. Leaky ReLU，PReLU，RReLU
5. ELU、SELU
6. GELU
7. Swish
8. Mish
9. ACON
10. Hybrid DNN
