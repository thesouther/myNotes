# 0.1 强化学习-DPG

paper:[Deterministic Policy Gradient Algorithms](https://hal.inria.fr/file/index/docid/938992/filename/dpg-icml2014.pdf)

核心:
- 对于连续动作空间的RL问题，提出确定性策略梯度算法.将其表示成action-value function的期望的梯度, 比随即策略梯度算法效率更高.
- 同时为了保证足够的探索, 提出off-policy的AC算法框架,从探索行行为策略中学习确定性的目标策略.

## 1 总述

策略梯度算法的基本想法是，使用一个参数化概率分布表示策略， 并使用它来选择动作, $$\pi_\theta(a|s)=\mathbb{P}[a|s;\theta]$$. PG算法通过抽样**随机策略**, 朝增大累积概率的方向调整策略参数.

**那么确定性策略梯度是否可以采用和随机策略相同的方法,沿着策略梯度的方向更新参数呢?** 本文证明确定性策略梯度存在, 并且当策略方差趋向于0 时, 确定性策略梯度是随机策略梯度的特例.

DPG只探索策略空间,需要的样本量比随机策略少很多. 但是为了对状态和动作空间进行完全探索, 往往需要随机策略. 本文使用**off-policy算法**. 基本思想是: **使用随机行动策略选择动作, 使用确定性目标策略进行学习**, 使用off-policy AC框架, 先逼近action-value函数, 然后更新策略网络参数.

## 2. 背景知识

### 2.1 基础

- MDP定义;
- 性能函数: $$J(\pi) = \mathbb{E}[r_1^\gamma | \pi] $$
- 将带折扣的状态分布表示成 $$\rho^\pi(s') := \int_\mathcal{S}\sum_{t=1}^\infty \gamma^{t-1} p_1(s)p(s\rightarrow s',t,\pi) ds $$, 可以把性能函数表示为
  
    <table>
        <tr>
             <th><img src="img/2020-12-08-22-07-33.png" ></th>
            <th> (1) </th>
        </tr>
    </table>
    
### 2.2 随机策略梯度(连续动作空间)

基本思想,沿着性能函数梯度的方向更新策略参数.

<table>
    <tr>
         <th><img src="img/2020-12-08-22-12-19.png" ></th>
        <th> (2) </th>
    </tr>
</table>

上式很简单, 尽管状态分布取决于策略参数, 但是策略梯度与状态分布的梯度无关. 重点是如何估计动作值函数Q.

### 2.3 随机AC 算法

actor对公式(2)使用随机梯度上升,更新随机策略$$\pi_\theta(s)$$的参数$$\theta$$. 使用参数$$w$$参数化值函数$$Q^w(s,a)$$来近似真实Q值. critic使用策略评估算法(TD等)估计$$Q^w \approx Q^\pi$$.

使用$$Q^w(s,a)$$会引入误差, 但是如果其满足:

| ![](img/2020-12-08-22-28-48.png) |
| :------------------------------: |
|              fig 1               |

- 条件i的意思是, compatible的函数逼近器在随机策略的“特征”中是线性的;
- 条件ii要求, 参数是根据这些特种估计$$Q^\pi$$的线性回归问题的解. 实际问题中该限制可以放松, 保证TD等算法可以更有效地进行策略评估. 当条件i和ii都满足, 整个算法等价于不使用critic.

### 2.4 off-policy AC


## 讨论

DPG算法好处:
- 随机策略梯度算法在状态空间和动作空间探索, 确定性策略梯度算法只在状态空间探索, 需要的样本量少很多.
- 计算性能:每次更新的计算量在动作维数和策略参数数量上是线性的.